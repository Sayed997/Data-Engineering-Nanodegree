{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Redshift Cluster for Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load DataWarehouse Params from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create clients for IAM, EC2, S3 and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create an IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Confirm cluster is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Display Cluster Endoint and ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Open an incoming TCP port to access cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-0be3d831')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[1]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preview log file in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-01-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-02-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-03-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-04-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-05-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-06-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-07-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-08-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-09-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-10-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-11-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-12-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-13-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-14-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-15-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-16-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-17-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-18-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-19-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-20-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-21-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-22-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-23-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-24-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-25-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-26-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-27-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-28-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-29-events.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log-data/2018/11/2018-11-30-events.json')\n"
     ]
    }
   ],
   "source": [
    "sparkifyBucket =  s3.Bucket(\"udacity-dend\")\n",
    "for obj in sparkifyBucket.objects.filter(Prefix=\"log-data\"):\n",
    "    print(obj) ## Both files have a common prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Warning: Printing this line will result in too large of an output \n",
    "#for obj in sparkifyBucket.objects.filter(Prefix=\"song-data\"):\n",
    "    #print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " #### Connect to a Redshift Cluster  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Test queries to make sure data warehouse is running correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>level</th>\n",
       "        <th>title</th>\n",
       "        <th>year</th>\n",
       "        <th>start_time</th>\n",
       "        <th>first_name</th>\n",
       "        <th>last_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>paid</td>\n",
       "        <td>The Moon Asked The Crow</td>\n",
       "        <td>2010</td>\n",
       "        <td>2018-11-29 17:01:34</td>\n",
       "        <td>Chloe</td>\n",
       "        <td>Cuevas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>paid</td>\n",
       "        <td>Hey Daddy (Daddy&#x27;s Home)</td>\n",
       "        <td>2010</td>\n",
       "        <td>2018-11-26 13:55:17</td>\n",
       "        <td>Hayden</td>\n",
       "        <td>Brock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>paid</td>\n",
       "        <td>One More Lie (Standing In The Shadows)</td>\n",
       "        <td>2010</td>\n",
       "        <td>2018-11-21 20:04:04</td>\n",
       "        <td>Kate</td>\n",
       "        <td>Harrell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>paid</td>\n",
       "        <td>Drunk Girls</td>\n",
       "        <td>2010</td>\n",
       "        <td>2018-11-09 19:34:24</td>\n",
       "        <td>Tegan</td>\n",
       "        <td>Levine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>paid</td>\n",
       "        <td>Hey Daddy (Daddy&#x27;s Home)</td>\n",
       "        <td>2010</td>\n",
       "        <td>2018-11-08 07:41:08</td>\n",
       "        <td>Tegan</td>\n",
       "        <td>Levine</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('paid', 'The Moon Asked The Crow', 2010, datetime.datetime(2018, 11, 29, 17, 1, 34), 'Chloe', 'Cuevas'),\n",
       " ('paid', \"Hey Daddy (Daddy's Home)\", 2010, datetime.datetime(2018, 11, 26, 13, 55, 17), 'Hayden', 'Brock'),\n",
       " ('paid', 'One More Lie (Standing In The Shadows)', 2010, datetime.datetime(2018, 11, 21, 20, 4, 4), 'Kate', 'Harrell'),\n",
       " ('paid', 'Drunk Girls', 2010, datetime.datetime(2018, 11, 9, 19, 34, 24), 'Tegan', 'Levine'),\n",
       " ('paid', \"Hey Daddy (Daddy's Home)\", 2010, datetime.datetime(2018, 11, 8, 7, 41, 8), 'Tegan', 'Levine')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT fs.level, ds.title, ds.year, fs.start_time, du.first_name, du.last_name\n",
    "FROM factSongplays fs\n",
    "JOIN dimSongs ds ON (fs.song_id = ds.song_id)\n",
    "JOIN dimUsers du ON (fs.user_id = du.user_id)\n",
    "ORDER BY ds.year DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>start_time</th>\n",
       "        <th>hour</th>\n",
       "        <th>day</th>\n",
       "        <th>week</th>\n",
       "        <th>month</th>\n",
       "        <th>year</th>\n",
       "        <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2018-11-01 21:01:46.796000</td>\n",
       "        <td>21</td>\n",
       "        <td>1</td>\n",
       "        <td>44</td>\n",
       "        <td>11</td>\n",
       "        <td>2018</td>\n",
       "        <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2018-11-01 21:05:52.796000</td>\n",
       "        <td>21</td>\n",
       "        <td>1</td>\n",
       "        <td>44</td>\n",
       "        <td>11</td>\n",
       "        <td>2018</td>\n",
       "        <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2018-11-02 09:01:21.796000</td>\n",
       "        <td>9</td>\n",
       "        <td>2</td>\n",
       "        <td>44</td>\n",
       "        <td>11</td>\n",
       "        <td>2018</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2018-11-02 09:16:16.796000</td>\n",
       "        <td>9</td>\n",
       "        <td>2</td>\n",
       "        <td>44</td>\n",
       "        <td>11</td>\n",
       "        <td>2018</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2018-11-02 09:57:56.796000</td>\n",
       "        <td>9</td>\n",
       "        <td>2</td>\n",
       "        <td>44</td>\n",
       "        <td>11</td>\n",
       "        <td>2018</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(datetime.datetime(2018, 11, 1, 21, 1, 46, 796000), 21, 1, 44, 11, 2018, 4),\n",
       " (datetime.datetime(2018, 11, 1, 21, 5, 52, 796000), 21, 1, 44, 11, 2018, 4),\n",
       " (datetime.datetime(2018, 11, 2, 9, 1, 21, 796000), 9, 2, 44, 11, 2018, 5),\n",
       " (datetime.datetime(2018, 11, 2, 9, 16, 16, 796000), 9, 2, 44, 11, 2018, 5),\n",
       " (datetime.datetime(2018, 11, 2, 9, 57, 56, 796000), 9, 2, 44, 11, 2018, 5)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dimTime\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      " * postgresql://dwhuser:***@dwhcluster.cjvrl59wfczu.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n",
      "nEvents\t\t= 8056\n",
      "nStaged_songs\t= 14896\n",
      "nSongplays\t= 333\n",
      "nUsers\t= 104\n",
      "nSongs\t\t= 14896\n",
      "nArtists\t\t= 10025\n",
      "nTime\t\t= 6813\n"
     ]
    }
   ],
   "source": [
    "nEvents = %sql select count(*) from staging_events;\n",
    "nStaged_songs = %sql select count(*) from staging_songs;\n",
    "nSongplays = %sql select count(*) from factSongplays;\n",
    "nUsers = %sql select count(*) from dimUsers;\n",
    "nSongs = %sql select count(*) from dimSongs;\n",
    "nArtists = %sql select count(*) from dimArtists;\n",
    "nTime = %sql select count(*) from dimTime;\n",
    "\n",
    "print(\"nEvents\\t\\t=\", nEvents[0][0])\n",
    "print(\"nStaged_songs\\t=\", nStaged_songs[0][0])\n",
    "print(\"nSongplays\\t=\", nSongplays[0][0])\n",
    "print(\"nUsers\\t=\", nUsers[0][0])\n",
    "print(\"nSongs\\t\\t=\", nSongs[0][0])\n",
    "print(\"nArtists\\t\\t=\", nArtists[0][0])\n",
    "print(\"nTime\\t\\t=\", nTime[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)#run until cluster is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1c55afd3-1689-4d9a-b7ab-920817f2cbb1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1c55afd3-1689-4d9a-b7ab-920817f2cbb1',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '200',\n",
       "   'date': 'Sat, 10 Apr 2021 19:59:56 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
